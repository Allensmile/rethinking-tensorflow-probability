{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ksachdeva/rethinking-tensorflow-probability/blob/master/notebooks/06_the_haunted_dag_and_the_causal_terror.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - The Haunted Dag and The Causal Terror (Work in Progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages that are not installed in colab\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    %tensorflow_version 2.X\n",
    "    \n",
    "    !pip install --upgrade daft\n",
    "    !pip install causalgraphicalmodels\n",
    "    !pip install watermark\n",
    "    !pip install arviz\n",
    "    !pip install tensorflow_probability==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import scipy.stats as stats\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.interpolate import BSpline\n",
    "\n",
    "# visualization \n",
    "import daft\n",
    "import matplotlib.pyplot as plt\n",
    "from causalgraphicalmodels import CausalGraphicalModel\n",
    "\n",
    "\n",
    "# aliases\n",
    "tfd = tfp.distributions\n",
    "Root = tfd.JointDistributionCoroutine.Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -p numpy,tensorflow,tensorflow_probability,arviz,scipy,pandas,daft,causalgraphicalmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config of various plotting libraries\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use('arviz-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow MCMC sampling helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to run MCMC Sampling\n",
    "def trace_to_arviz(trace, sample_stats=None):\n",
    "    posterior = {k: v.numpy() for k, v in trace.items()}\n",
    "    if sample_stats:\n",
    "        sample_stats = {k: v.numpy().T for k, v in sample_stats.items()}\n",
    "    return az.from_dict(posterior=posterior, sample_stats=sample_stats)\n",
    "\n",
    "@tf.function\n",
    "def run_hmc_chain(log_posterior, inits=dict(), step_size=1, burn_in=2000, num_samples=5000):\n",
    "    \n",
    "  adaptation_steps = int(0.5 * burn_in)\n",
    "  \n",
    "  hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "      # The actual HMC is very simple to define\n",
    "      tfp.mcmc.HamiltonianMonteCarlo(\n",
    "          target_log_prob_fn = log_posterior,    # Log Posterior goes here\n",
    "          num_leapfrog_steps = 3,\n",
    "          step_size = step_size                  # constant step size\n",
    "      ),\n",
    "      num_adaptation_steps = adaptation_steps\n",
    "  )\n",
    "    \n",
    "  results = tfp.mcmc.sample_chain(\n",
    "      num_results = num_samples,\n",
    "      num_burnin_steps = burn_in,\n",
    "      current_state = inits.values(),\n",
    "      kernel = hmc,\n",
    "      trace_fn=None\n",
    "  )\n",
    "  return results\n",
    "\n",
    "def sample_mcmc_hmc(log_posterior, inits=dict(), step_size=0.1, burn_in=1000, num_samples=1000):\n",
    "    results = run_hmc_chain(log_posterior, inits, step_size, burn_in, num_samples)\n",
    "    # convert results to numpy arrays for easy processing later on\n",
    "    numpy_results = list(map(lambda r : r.numpy(), results))\n",
    "    posterior = dict(zip(list(inits.keys()), numpy_results))    \n",
    "    return posterior\n",
    "\n",
    "def sample_from_posterior(jdc, observed_data, params, step_size=0.1):    \n",
    "    \"\"\" Helper function to sample from the posterior distribution given the observed value\"\"\"\n",
    "    \n",
    "    # we get the sample from our model definition\n",
    "    # that will act as the init state for the chain\n",
    "    samples = jdc.sample()    \n",
    "    samples_without_outcome = samples[:-1]\n",
    "    \n",
    "    # build a dicitonary using the params\n",
    "    init_state = dict(zip(params, samples_without_outcome))\n",
    "\n",
    "    target_log_prob_fn = lambda *x: jdc.log_prob(x + (observed_data,))\n",
    "\n",
    "    return sample_mcmc_hmc(target_log_prob_fn,inits=init_state,step_size=step_size)\n",
    "    \n",
    "def squeeze_post(post):\n",
    "    return {k : v.squeeze() for k, v in post.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could change base url to local dir or a remoate raw github content\n",
    "_BASE_URL = \"https://raw.githubusercontent.com/rmcelreath/rethinking/Experimental/data\"\n",
    "\n",
    "WAFFLE_DIVORCE_DATASET_PATH = f\"{_BASE_URL}/WaffleDivorce.csv\"\n",
    "MILK_DATASET_PATH = f\"{_BASE_URL}/milk.csv\"\n",
    "HOWELL_DATASET_PATH = f\"{_BASE_URL}/Howell1.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.1\n",
    "\n",
    "\n",
    "**Berkson's paradox** - A *false* observation of a *negative* coorelation between 2 positive traits.\n",
    "\n",
    "Members of a population which have some positive trait tend to lack a second even though -:\n",
    "\n",
    "* The traits may be unrelated\n",
    "\n",
    "* Or, they may be even positively related.\n",
    "\n",
    "e.g Resturants at good location have bad food even though location & food have no correlation\n",
    "\n",
    "Author give another name to **Berkon's paradox**. He calls it **selection-distortion effect**.\n",
    "\n",
    "The gist of the idea here is that when a sample is selected on a combination of 2 (or more) variables, the relationship between those 2 variables is different after selection than it was before. \n",
    "\n",
    "\n",
    "Because of above, author suggests that we should always be cautious of adding more predictor variables to our regression as it may introduce statistical selection with in the model. The phenomenon has a name and it is called **COLLIDER BIAS**.\n",
    "\n",
    "He suggests to look at the **causal model** as the remedy.\n",
    "\n",
    "There are actually 3 types of hazards when we add more predictor variables -\n",
    "\n",
    "* Multicollinearity \n",
    "* Post treatment bias\n",
    "* Collider bias\n",
    "\n",
    "Collider - a beam in which two particles are made to collide (collision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simulated example to demonstrate selection-disortion \n",
    "\n",
    "_SEED = 1914\n",
    "\n",
    "N = 200\n",
    "p = 0.1  # proportion to select\n",
    "\n",
    "# uncorrelated newsworthiness & trustworthiness\n",
    "seed = tfp.util.SeedStream(_SEED, salt=\"\")\n",
    "nw = tfd.Normal(loc=0., scale=1.).sample(N, seed=seed()).numpy()\n",
    "seed = tfp.util.SeedStream(_SEED, salt=\"\")\n",
    "tw = tfd.Normal(loc=0., scale=1.).sample(N, seed=seed()).numpy()\n",
    "\n",
    "# select top 10% of combined scores\n",
    "s = nw + tw  # total score\n",
    "q = np.quantile(s, 1 - p)  # top 10% threshold\n",
    "\n",
    "selected = np.where(s >= q, True, False)\n",
    "\n",
    "np.corrcoef(tw[selected], nw[selected])[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.2\n",
    "\n",
    "\n",
    "**Multicollinearity** - when the predictor variables are stronly correlated.\n",
    "\n",
    "When this happens the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome. Author says that this happens because of how multiple regression work !!. \n",
    "\n",
    "\n",
    "TODO - Investigate above point ie. how multiple regression leads to this.\n",
    "\n",
    "Author suggest that the model will still infer correct but it would be difficult to understand it.\n",
    "\n",
    "Here we again create an artifical dataset about height & its relation to the lenght of the legs as predictor variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SEED = 909\n",
    "\n",
    "N = 100\n",
    "\n",
    "def generate_height_leg_data():\n",
    "    seed = tfp.util.SeedStream(_SEED, salt=\"leg_exp\")\n",
    "    height = tfd.Normal(loc=10., scale=2.).sample(N, seed=seed()).numpy()\n",
    "    leg_prop = tfd.Uniform(low=0.4, high=0.5).sample(N, seed=seed()).numpy()\n",
    "\n",
    "    # left & right leg as proportion + error\n",
    "    leg_left = leg_prop * height + tfd.Normal(loc=0, scale=0.02).sample(N, seed=seed()).numpy()\n",
    "\n",
    "    leg_right = leg_prop * height + tfd.Normal(loc=0, scale=0.02).sample(N, seed=seed()).numpy()\n",
    "\n",
    "    # build a dataframe using above\n",
    "    d = pd.DataFrame({\n",
    "        \"height\" : height,\n",
    "        \"leg_left\" : leg_left,\n",
    "        \"leg_right\" : leg_right\n",
    "    })\n",
    "\n",
    "    return d\n",
    "\n",
    "d = generate_height_leg_data()\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.3 (not working !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6_1(leg_left_data, leg_right_data):\n",
    "    def _generator():\n",
    "      alpha = yield Root(tfd.Sample(tfd.Normal(loc=10., scale=100., name=\"alpha\"), sample_shape=1))\n",
    "      betaL = yield Root(tfd.Sample(tfd.Normal(loc=2., scale=10., name=\"betaL\"), sample_shape=1))\n",
    "      betaR = yield Root(tfd.Sample(tfd.Normal(loc=2., scale=10., name=\"betaR\"), sample_shape=1))\n",
    "      sigma = yield Root(tfd.Sample(tfd.Exponential(rate=1., name=\"sigma\"), sample_shape=1))\n",
    "    \n",
    "      mu =  alpha + betaL * leg_left_data + betaR * leg_right_data\n",
    "        \n",
    "      height = yield tfd.Independent(tfd.Normal(loc=mu, scale=sigma, name=\"height\"), reinterpreted_batch_ndims=1)\n",
    "\n",
    "    return tfd.JointDistributionCoroutine(_generator, validate_args=True)    \n",
    "    \n",
    "jdc_6_1 = model_6_1(d.leg_left.values, d.leg_right.values)\n",
    "\n",
    "posterior_6_1 = sample_from_posterior(\n",
    "                    jdc_6_1, \n",
    "                    observed_data=d.height.values, \n",
    "                    params=['alpha', 'betaL', 'betaR', 'sigma'])\n",
    "\n",
    "az_trace = az.from_dict(posterior=posterior_6_1, sample_stats=None)\n",
    "\n",
    "az.summary(az_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_post = squeeze_post(posterior_6_1)\n",
    "\n",
    "az.plot_forest(np_post, credible_interval=0.89);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.5  (not working !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(np_post, [\"betaR\", \"betaL\"], plot_kwargs={\"alpha\": 0.1});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.6  (not working !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_blbr = np_post[\"betaL\"] + np_post[\"betaR\"]\n",
    "az.plot_kde(sum_blbr, label=\"sum of bl and br\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.8\n",
    "\n",
    "**Multicollinear milk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(MILK_DATASET_PATH, sep=\";\")\n",
    "\n",
    "d[\"K\"] = d[\"kcal.per.g\"].pipe(lambda x: (x - x.mean()) / x.std())\n",
    "d[\"F\"] = d[\"perc.fat\"].pipe(lambda x: (x - x.mean()) / x.std())\n",
    "d[\"L\"] = d[\"perc.lactose\"].pipe(lambda x: (x - x.mean()) / x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are building here 2 models. \n",
    "\n",
    "# KCal is regressed on perc.fat\n",
    "\n",
    "def model_6_3(per_fat):\n",
    "    def _generator():\n",
    "      alpha = yield Root(tfd.Sample(tfd.Normal(loc=0., scale=0.2, name=\"alpha\"), sample_shape=1))\n",
    "      betaF = yield Root(tfd.Sample(tfd.Normal(loc=0., scale=0.5, name=\"betaF\"), sample_shape=1))      \n",
    "      sigma = yield Root(tfd.Sample(tfd.Exponential(rate=1., name=\"sigma\"), sample_shape=1))\n",
    "    \n",
    "      mu =  alpha + betaF * per_fat\n",
    "        \n",
    "      K = yield tfd.Independent(tfd.Normal(loc=mu, scale=sigma, name=\"K\"), reinterpreted_batch_ndims=1)\n",
    "\n",
    "    return tfd.JointDistributionCoroutine(_generator, validate_args=True)    \n",
    "    \n",
    "jdc_6_3 = model_6_3(d.F.values)\n",
    "\n",
    "posterior_6_3 = sample_from_posterior(\n",
    "                    jdc_6_3, \n",
    "                    observed_data=d.K.values, \n",
    "                    params=['alpha', 'betaF', 'sigma'])\n",
    "                            \n",
    "                            \n",
    "# KCal is regressed on perc.lactose\n",
    "\n",
    "def model_6_4(per_lac):\n",
    "    def _generator():\n",
    "      alpha = yield Root(tfd.Sample(tfd.Normal(loc=0., scale=0.2, name=\"alpha\"), sample_shape=1))\n",
    "      betaL = yield Root(tfd.Sample(tfd.Normal(loc=0., scale=0.5, name=\"betaL\"), sample_shape=1))      \n",
    "      sigma = yield Root(tfd.Sample(tfd.Exponential(rate=1., name=\"sigma\"), sample_shape=1))\n",
    "    \n",
    "      mu =  alpha + betaL * per_lac\n",
    "        \n",
    "      K = yield tfd.Independent(tfd.Normal(loc=mu, scale=sigma, name=\"K\"), reinterpreted_batch_ndims=1)\n",
    "\n",
    "    return tfd.JointDistributionCoroutine(_generator, validate_args=True)    \n",
    "    \n",
    "jdc_6_4 = model_6_4(d.L.values)\n",
    "\n",
    "posterior_6_4 = sample_from_posterior(\n",
    "                    jdc_6_4, \n",
    "                    observed_data=d.K.values, \n",
    "                    params=['alpha', 'betaL', 'sigma'])\n",
    "\n",
    "\n",
    "az_trace_6_3 = az.from_dict(posterior=posterior_6_3, sample_stats=None)\n",
    "az_trace_6_4 = az.from_dict(posterior=posterior_6_4, sample_stats=None)\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(az.summary(az_trace_6_3))\n",
    "print(\"=\"*20)\n",
    "\n",
    "print(\"=\"*20)\n",
    "print(az.summary(az_trace_6_4))\n",
    "print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior for betaF & betaL are mirror images of each other (0.857 vs -0.898)\n",
    "\n",
    "This seems to imply that both predictors have strong association with the outcome. In next section we will see what happens when we combine both of them in the regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6_5(per_fat, per_lac):\n",
    "    def _generator():\n",
    "      alpha = yield Root(tfd.Sample(tfd.Normal(loc=0., scale=0.2, name=\"alpha\"), sample_shape=1))\n",
    "      betaF = yield Root(tfd.Sample(tfd.Normal(loc=0., scale=0.5, name=\"betaF\"), sample_shape=1))      \n",
    "      betaL = yield Root(tfd.Sample(tfd.Normal(loc=0., scale=0.5, name=\"betaL\"), sample_shape=1))      \n",
    "      sigma = yield Root(tfd.Sample(tfd.Exponential(rate=1., name=\"sigma\"), sample_shape=1))\n",
    "    \n",
    "      mu =  alpha + betaF * per_fat + betaL * per_lac\n",
    "        \n",
    "      K = yield tfd.Independent(tfd.Normal(loc=mu, scale=sigma, name=\"K\"), reinterpreted_batch_ndims=1)\n",
    "\n",
    "    return tfd.JointDistributionCoroutine(_generator, validate_args=True)    \n",
    "    \n",
    "jdc_6_5 = model_6_5(d.F.values, d.L.values)\n",
    "\n",
    "posterior_6_5 = sample_from_posterior(\n",
    "                    jdc_6_5, \n",
    "                    observed_data=d.K.values, \n",
    "                    params=['alpha', 'betaF', 'betaL', 'sigma'])\n",
    "\n",
    "az_trace_6_5 = az.from_dict(posterior=posterior_6_5, sample_stats=None)\n",
    "az.summary(az_trace_6_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the standard deviations of the posterior for betaF & betaL has jumped up significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.11\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_pair(d[[\"kcal.per.g\", \"perc.fat\", \"perc.lactose\"]].to_dict(\"list\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* perc.fat is positively related to outcome\n",
    "\n",
    "* perc.lac is negatively related to outcome\n",
    "\n",
    "* perc.fat & perc.lac are negatively coorelated to each other\n",
    "\n",
    "Either of them helps in predicting KCal but neither helps much once we already know the other.\n",
    "\n",
    "Author suggests that often before modelling people look at the correlation between variables & drop them if it is the case. He calls this a mistake and suggest that the pairwise correlations are not the problems. \n",
    "\n",
    "He argues that it is the conditional association - not correlations - that matter. \n",
    "\n",
    "Associations with in data alone are not enough to decide what to do. \n",
    "\n",
    "He motivates to look at the problem causally and mentions the existence of an unobserved variable. This type of variable is also called **Non identifiable variable** i.e. the structure of the data & the model does not make it possible to esitmate this parameter's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.12  (TODO)\n",
    "\n",
    "Simulating collinearity using Milk dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 6.13\n",
    "\n",
    "**Omitted Variable bias** - Problems that arise because of **not** including predictior variables\n",
    "\n",
    "**Post-treatment bias** - Problems that arise becuase of including **improper** predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
